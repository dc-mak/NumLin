\chapter{Implementation}\label{chap:impl}

\begin{guidance}
This chapter may be called something else\ldots but in general
the idea is that you have one (or a few) ``meat'' chapters which
describe the work you did in technical detail.
\end{guidance}

\prechapter{%
    I implemented LT4LA in OCaml, however I strongly believe the ideas
    described in this chapter can be applied easily to other languages and also
    are modular enough to extend the OCaml implementation to output to different
    back-end languages. I will show how a small core language with a few features
    can be the target of heavy-desugaring of typical linear-algebra programs. This
    core language can then be elaborated and checked for linearity before
    performing some simple and predictable optimisations and emitting (in this
    particular implementation) OCaml code that is not obviously safe and correct
    (with respect to linearity).
}%

\section{Structure of LT4LA}

LT4LA follows the structure of a typical compiler for a (E)DSL\@. From the
start, I made a concerted effort to (1) write pure-functional code (typically
using a monadic-style) which helped immensely with modularity and debugging
when tests showed errors (2) produce readable, useful and precise
error-messages in the hope that someone who did not understand linear types
could still use the LT4LA (3) write tests and set-up
continuous-integration for all non-trivial functions so that I could spot and
correct errors that were not caught by OCaml's type-system whenever I
implemented new features or or refactored my code.

\begin{enumerate}

    \item \textbf{Parsing \& Desugaring}. A generated, LR(1) parser parses a text
        file into a syntax tree, which is then desugared into a smaller, more
        concise abstract syntax tree. The former aims to mimic OCaml syntax
        with a few extensions and keywords so that it is familiar and thus easy
        to pick-up for OCaml users. The latter allows for the type-checker to
        be simpler to implement and easier to specify. In general, this part
        will vary for different languages or can be dealt with differently
        using combinators (the EDSL approach) or a syntax-extension if the host
        language offers such support.

    \item \textbf{Type-checking}. The abstract syntax tree is explicitly-typed,
        with some inference to make it less verbose and more convenient to
        write typical programs.

    \item \textbf{Matrix Expressions}. During type-checking, if a matrix-expression
        is encountered, it is either successfully elaborated into an expression
        in the abstract syntax tree which is then consequently type-checked, or
        fails to find suitable routines to calculate the given expression.

    \item \textbf{Code Generation}. The abstract syntax tree is translated into
        standard OCaml and a few-particular `optimisations' are made to produce
        more readable code. This process is type-preserving: the linear type
        system is embedded into OCaml's type system, and so when the OCaml compiler
        compiles the generated code, it acts as a sanity check on the code produced.

    \item \textbf{Executable Artifacts}. A transpiler and a REPL are the main
        artifacts produced for this thesis. For evaluation, I implemented
        Kalman filters in Owl, LT4LA and CBLAS/LAPACKE and a benchmarking
        program to measure execution times.

    \item \textbf{Tests}. As mentioned before, almost all non-trivial functions
        have tests to check their behaviour. The output of the transpiler was
        also tested by having the build system generate OCaml code at compile
        time, which in turn could then be compiled and tested like handwritten
        OCaml code.

\end{enumerate}

\section{Core Language}

A full description of the core language can be found in
Appendix~\ref{chap:ott_spec}. Its main features are intuitionistic values,
value-restriction, fractional-capabilities (inferrable at call sites),
if-expressions and recursion.

\subsection{Intuitionistic Values}

% Why, runtime representation, trade-offs etc.

To make a linearly-typed language usable, we need some way of using values zero
or more than once, as we would an intuitionistic value. For this, we have in
the type-expressions the !-constructor and in the term-expressions the
\ltfla{Many}-constructor and the \ltfla{let Many <id> = .. in .. } eliminator.
The idea behind the !-type is that the value uses no `resources'
(linearly-typed expressions). To start off with, it is enough to say that
anything which can be passed around by copying, will have a !-type. This
includes integers, elements and booleans. So \ltfla{3 : !int} and \ltfla{3. +.
4. : !elt}. However, all bindings are still linear by default, so to emulate
intuitionism, I desugar \ltfla{let !x = <exp> in <body>} to  \ltfla{let Many x
= <exp> in let Many x = Many (Many x) in <body>} (similarly for function
argument bindings). The reader can check using the rules in the Appendix that
this has the effect of moving \ltfla{x : !t} from the linear to the
intuitionistic environments, only if \ltfla{<exp> : !t}.

However, just that desugaring alone is not enough to prevent a user from taking
an array or matrix and moving it into the intuitionistic environments. Why?
There are certain situations in which we \emph{should not} use the \ltfla{Many}
constructor.  Consider the following code: \ltfla{let Many x = Many (array 5)
in <body>}; the expression \ltfla{array 5} uses no linearly-typed variables
from the linear environment. Although we could just reject types of the form
\ltfla{!(_ arr)} to fix this simple example, what about pairs \ltfla{let Many
xy = Many (3, array 5) in <body>}?  Ad-hoc pattern matching on the type cannot
account for all possible situations. With the last case, we can use \ltfla{xy}
as many times as we would like, destruct the pair to get the second component
and thus create \emph{distinct} read-write aliases to the same array.  Alas,
now arrays can be used intuitionistically and all the benefits of linearity are
lost.  Or are they?

\subsection{Value-Restriction}

Not quite, but to understand how we can fix this problem, we need to question
an assumption left implicit up until this point: what does \ltfla{Many} even
mean? What does it do at runtime? One option is to go down the C++ route and
make \ltfla{Many} act like a \ltfla{shared_ptr} and act as a runtime
reference-count for arrays. I chose to not go for this option because it went
against the \emph{explicitness} and \emph{predictability} that C and Fortran
have. It would make analysing when and what is allocated and freed more like
the higher-level languages I was trying to move away from.

My aim is to show linear types can be simple to understand and apply to linear
algebra, enough so that it can be grafted (in a limited way) on to existing
languages as a \emph{library}. In that spirit, the simplest thing that
\ltfla{Many} can mean at runtime is \emph{nothing}. This language construct is
translated into a standard OCaml language constructor of the form \ltfla{type
'a bang = Many of 'a [@@unboxed]}. The \emph{unboxed} annotation means
that the type and its constructor only exist for the purpose of type-checking
in OCaml; \emph{the runtime representation of values of type \ltfla{'a} and
\ltfla{'a bang} is exactly the same}.

With this understanding, our problem is that arrays and matrices are unlike
other values such as integers and elements because (under the OCaml hood)
calling a function with an array argument copies a \emph{pointer} to the array
rather than the array itself, instead of the \emph{value} itself. So, we can
start making a distinction, \emph{defining} elements, integers, booleans,
intuitionistic variables, units and lambda-expressions that capture no linear
variables as \emph{values} (since they cannot break referential-transparency)
and anything else (arrays, matrices, expressions which can be reduced, such as
function application and if-expressions) as not being `values'. If this sounds
familiar, it is because this is the same \emph{value-restriction} `trick' from
the world of polymorphic types applied to linearity instead. We then have the
rule that we can only use \ltfla{Many} on expressions that are defined to be
\emph{values} and \emph{use no linear variables}.

\subsection{Fractional-Capabilities and Inference}

Having understood and correctly implemented intuitionism, we now tackle
the problem of how to implement safe aliasing, that is taking a read-write
alias and splitting into read-only aliases, as well as the inverse.

Array and matrix types are parameterised by \emph{fractional-capabilities}.  A
fraction of 1 ($2^0$) represents complete ownership of a value; in particular,
this allows a programmer to write or free it. Creating an array gives you
ownership of it; the function \ltfla{array : !int --o z arr} (where \ltfla{z}
represents `0'). Once you have ownership of an array, you can free it:
\ltfla{free : z arr --o unit}.  Importantly, because a linear-value may only be
used once, the array just freed is \emph{out of scope} for following
expressions, preventing use-after-free.  Ownership also enables you to write to
the array: \ltfla{set : z arr --o !int --o !elt --o z arr} (the syntax
\ltfla{w[i] := j} is just sugar for \ltfla{set w i j}). Here, linearity
prevents accessing aliases which represented the array \emph{before} the
mutation.

Any fraction less than 1 (for simplicity, limited to $2^{-k}$ in this system,
for a positive integer $k$) represents read-only access. So, the \ltfla{'x}
represents a natural number (either a zero \ltfla{z}, variable \ltfla{'x} or a
successor (+1) of a natural number). Hence, you can read from (index) any array
\ltfla{get : 'x .  'x arr --o !int --o 'x arr * !elt} (the syntax \ltfla{let !v
<- w[i]} is just sugar for \ltfla{let (w, !v) = get _ w i}). Transparently
rebinding \ltfla{w} with the returned value means a program can appear to use
\ltfla{w} multiple times. The underscore is how a programmer tells the compiler
to automatically \emph{infer} the correct fractional-capability based on the
other arguments passed to the function. In conjunction with the requirement
that functions declarations need type-annotations for their arguments, this
allows a fractional-capability to be correctly inferred in any program.

Fractions exist solely to enable safe aliasing, via the primitive \ltfla{share
: 'x . 'x arr --o 'x s arr * 'x s arr}.  The two arrays returned (which happen
to just be the given array) can now only be read from and not written to.  If
you want to write to this array, you must \ltfla{unshare: 'x .  'x s arr --o
'x s arr --o 'x arr} it with other read-only aliases until you are left with a
value of type \ltfla{z arr}, guaranteeing no other aliases exists.

Given this set-up, we now \emph{statically} have \emph{perfect} information
about aliasing and ownership of values in the program. We can only write to an
array when we own it; ownership guarantees no other aliases exist in scope at
the point of usage. In Figure~\ref{fig:ltfla_kalman}, I show how this perfect
information can be used to write more natural-looking code using value-semantic
expressions which behave in precisely the way we intend it to.  Now the
programmer need not resort to manually tracking and inserting \texttt{noalias}
annotations; instead they can let the loyal and tireless compiler do the heavy
lifting.

\subsection{If-Expressions}

Because we do not know which way a condition will evaluate at run time, we must
guarantee the both branches use the same set of linear variables. Writing the
type-checker in a pure-functional monadic style paid off here because I could
now sandwich monadic values with state-adjustments either side of it. Given two
monadic values that represented type-checking two branches of an if-expression,
I could use the code in Figure~\ref{fig:same_resources} to easily save, reset
and compare the state either side of running those monadic values.

\begin{figure}[tp]
    \begin{minted}{ocaml}
let same_resources (wf_a, loc_a) (wf_b, loc_b) =
  let open Let_syntax in
  (* Save state *)
  let%bind {used_vars=prev; env=old_env; _} as state = get in
  (* Reset, run a, save state *)
  let%bind () = put { state with used_vars = empty_used } in
  let%bind res_a = wf_a in
  let%bind {used_vars=used_a; _} as state = get in
  (* Reset, run b, save state *)
  let%bind () = put { state with used_vars = empty_used; env = old_env } in
  let%bind res_b = wf_b in
  let%bind {used_vars=used_b; _} as state = get in
  (* Check if same resources *)
  let keys_a, keys_b = (* convert to (used_a, used_b) to sets *) in
  if Set.equal keys_a keys_b then
    (* merge used_vars and used_b environments *)
  else
    (* report differences *)
    \end{minted}

    \caption{Implementation of same\_resources helper method for type-checking
        if-expressions. Note the monadic style helped compose computations that
        affected the type-checker's state in a simple manner.}\label{fig:same_resources}

\end{figure}

\subsection{Functions and Recursion}

A non-recursive function may be used more than once if it does not refer to any
linear variables from the surrounding scope. So, we can desugar something like
\ltfla{let x = 3 in let !f (y : !int) = x + y in <body>} to \ltfla{let x = 3 in
let Many f =  Many (fun y : !int -> x + y) in  <body>}. Recursion is
slightly more complicated: we can desugar the factorial function
(Figure~\ref{fig:ltfla_factorial}) to \ltfla{let Many f = fix (f, x : !int,
if (*..*) : !int) in <body> }. However, \ltfla{fix}, like \ltfla{Many},
must also not use any linear variables from its surrounding scope.

\section{Matrix Expressions}

We have now arrived at the titular \emph{applications} of linear types. I will
show how, in addition to preventing the errors explained hitherto, we can take
linear types one step further and apply it to the domain of efficient
compilation of matrix-expressions.

\begin{sidewaysfigure}
    \inputminted[linenos, fontsize=\footnotesize]{fortran}{kalman.f90}
    \caption{Kalman filter in Fortran 90.}\label{fig:fortran_kalman}
\end{sidewaysfigure}

In Figure~\ref{fig:fortran_kalman}, we see the difficulty of efficiently
implementing a \emph{Kalman filter}, a powerful set of equations applicable
to a wide variety of problems. From the comments, we see that every variable is
annotated with the step/matrix expression that it will hold at some point
during the computation (an equivalent alternative, say in C++, could be to have
a meaningful name for each step/matrix expression and manually annotate/keep
track of which names alias the same location).

In contrast, Figure~\ref{fig:ltfla_kalman}, offers the advantages of
\begin{itemize}
    \item aliasing: labelling each step with a different, more meaningful variable name,
    \item easily spotting which resources are being passed in and which are
        allocated for the function (new/copy),
    \item unambiguously seeing \emph{when} and what values are freed;
\end{itemize}
and have the compiler automatically ensure the safety of each of the above by respectively
\begin{itemize}
    \item making it impossible to refer to steps/values which are no longer usable,
    \item ensuring all values are declared and \emph{initialised} correctly before they are used,
    \item checking no values are used after they are freed \emph{or} leaked.
\end{itemize}

Indeed, an inexperienced programmer could take the  na\"ive approach of just
copying sub-expressions by default and then letting the compiler tell it which
copies are never used and removing them systematically until it type-checks.
While it is not quite a black-box, push-button compilation of an expression, I
would argue that, it is just as easy (if not easier) to become familiar with as
Rust and its borrow-checker.

\begin{figure}[tb]
    \inputminted[linenos, fontsize=\small]{ocaml}{../test/examples/kalman.lt}
    \caption{Kalman filter in LT4LA.}\label{fig:ltfla_kalman}
\end{figure}

\subsection{TO DO: Elaboration}

So-called `matrix \emph{expressions}' are still `side-effecting'/consuming
linear variables, and do not produce \emph{values} but a \emph{sequence of
(re-)bindings} which in turn dictate what values are still available/in-scope
after the computation.  As such, compilation cannot be done purely
compositionally via structural-induction on the expression language; it is most
concisely expressed via CPS, where the result of elaborating a matrix
expression is a function that takes as its argument the rest of the computation
expecting to use the result it has just made available under that requested
name.

\section{TO DO: Code Generation}

High-level overview: primitives, translations, optimisations, difficulty, build system.

\subsection{TO DO: Compiling Constructs to OCaml}

\subsection{TO DO: Metaprogramming}
