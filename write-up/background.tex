\chapter{Background}

% \begin{guidance}
%     A more extensive coverage of what's required to understand your
%     work. In general you should assume the reader has a good undergraduate
%     degree in computer science, but is not necessarily an expert in
%     the particular area you've been working on. Hence this chapter
%     may need to summarize some ``text book'' material.
%
%     This is not something you'd normally require in an academic paper,
%     and it may not be appropriate for your particular circumstances.
%     Indeed, in some cases it's possible to cover all of the ``background''
%     material either in the introduction or at appropriate places in
%     the rest of the dissertation.
% \end{guidance}

\prechapter{%
    I will outline the concept of linear types and show how they can be used to
    solve the problems faced by programmers writing code using linear-algebra
    libraries. I will emphasise \emph{practical} and intuitive explanations of
    linear types to keep this thesis accessible to programmers as well as
    academics not familiar with type-theory and will give only a terse overview
    of the history and theory behind linear types for the interested reader to
    pursue further.
}%

\section{Tracking Resources with Linearity}
Familiar examples of using a type-system to express program invariants are
existential-types for abstraction and encapsulation, polymorphic types for
parametricity and composition (a.k.a generics). Less-known examples include
dependent-types (contracts or pre- and post-conditions). The advantages of
using a type-system to express program invariants are summarised by saying the
stronger the rules you follow, the better the guarantees you can get about your
program, \emph{before} you run it. At first, the rules seem restrictive, but
similar to how the rules of grammar, spelling and more generally writing help a
writer make it easier and clearer to communicate the ideas they wish to express,
so too do typing rules make it easier to communicate the intent and
assumptions under which a program is written. An added, but often overlooked
benefit is automated-checking: a programmer can boldly refactor in certain
ways and the compiler will \emph{assist} in ensuring the relevant invariants
the type-system enforces are updated and kept consistent by pointing-out
where they are violated.

Linear types are a way to help a programmer track and manage resources. In
practical programming terms, they enforce the restriction that a value may be
used exactly once\footnote{This definition may differ from more colloquial uses
in discussions surrounding \emph{substructural} type systems and/or Rust.}
(Figure~\ref{fig:example_rules} shows two example typing rules for a typical,
linearly typed language). While this restriction may seem limiting at first,
precisely these constraints can be used to express common invariants of the
programs written by programmers every day. For example: a file or a
socket, once opened \emph{must} closed; all memory that is manually allocated
\emph{must} be freed.  C++'s destructors and Rust's Drop-trait (and more
generally, its borrow-checker) attempt to enforce these constraints by
basically doing the same thing: any resource that has not been moved is
deallocated at the end of the current lexical scope. Notably, these languages
also permit aliasing, alongside rules enforcing when it is acceptable to do so.
On face value, the above one-line description of linear types prevents aliasing
or functions such as $\lambda x.\; (x, x)$; such features are still allowed
(albeit in a more restricted fashion) in a \emph{usable} linear type system
designed for working with linear-algebra libraries.

\begin{figure}
    \centering
    \begin{minipage}{0.3\textwidth}
        \centering
    \begin{prooftree}
        \AxiomC{}
        \RightLabel{\textsc{Var}}
        \UnaryInfC{$\Gamma, x : t \vdash x : t ; \Gamma$}
    \end{prooftree}
    \end{minipage}%
    \begin{minipage}{0.6\textwidth}
        \centering
    \begin{prooftree}
        \AxiomC{$\Gamma_1 \vdash e_1 : t_1 ; \Gamma_2$}
        \AxiomC{$\Gamma_2 \vdash e_2 : t_2 ; \Gamma_3$}
        \RightLabel{\textsc{Pair}}
        \BinaryInfC{$\Gamma_1 \vdash (e_1, e_2) : t_1 \otimes t_2 ; \Gamma_3$}
    \end{prooftree}
    \end{minipage}
    \caption{A typing rule has the general form $\Gamma_{\textrm{in}} \vdash e ; \Gamma_{\textrm{out}}$.
    Typing a variable \emph{removes} it from the environment. Typing a pair
    requires that each component of the pair uses \emph{different} resources
    from the environment.}\label{fig:example_rules}
\end{figure}

\section{Problem in Detail}

Given this background, the most pertinent question at hand is: what problems do
linear-algebra library users (and writers) typically face? The answer to this
question depends on which of two buckets a programmer falls (or is forced by
domain) into. On one side, we have users of high-level linear-algebra libraries such
as Owl (for OCaml), Julia and Numpy (for Python); other the other, we have
users of more manual, lower-level libraries such as BLAS (Basic Linear Algebra
Subroutines) for languages like C++ and FORTRAN.\footnote{I am not including
Rust in this comparison because its linear-algebra libraries are under active
development and not as well-known/used. Later on, given that it is a language
with in-built support of substructural features to track resources, Rust will
be compared and contrasted with this project to evaluate the classic
(E)DSL-versus-language-feature debate as it applies to the domain of
linear-algebra libraries.} Most of what follows applies to \emph{dense}
linear-algebra computations rather than \emph{sparse} because memory
allocated for results typically depends on the sparsity of the inputs and
so is not immediately amenable to the techniques proposed in this thesis.

\subsection{One Too Many Copies and a Thousand Bytes Behind}
\begin{figure}[tbp]
    \centering
    \begin{minipage}{0.45\textwidth}
    \centering
    \begin{minted}[fontsize=\small]{python}
# Numpy (Python)
import numpy.matlib
a = [[1,0],[0,1]]
b = [[4,1],[2,2]]
c = numpy.matmul(a,b)
    \end{minted}
    \begin{minted}[fontsize=\small]{julia}
# Julia
c = [1 0; 0 1] * [4 1; 2 2]
    \end{minted}
    \end{minipage}%
    \begin{minipage}{0.45\textwidth}
    \centering
    \begin{minted}[fontsize=\small]{ocaml}
(* Owl (OCaml) *)
open Owl
let a = Mat.of_arrays
  [|[|1.;0.|];[|0.;1.|]|]
let b = Mat.of_arrays
  [|[|4.;1.|];[|1.;2.|]|]
let c = Mat.(a *@ b)
    \end{minted}
    \end{minipage}
    \caption{Matrix Multiplication in Numpy (Python), Julia and Owl (OCaml).}\label{fig:mat_mul_copies}
\end{figure}

In Figure~\ref{fig:mat_mul_copies}, we see that matrix-multiplication is fairly
trivial to write and execute in Numpy, Julia and Owl. Let us call this approach
\emph{value-semantic}, meaning that objects are \emph{values} just like
integers and floating-point numbers. This approach confers two key advantages to the
programmmer: it is easy to read (equational and algebraic expressions) and it
is easy to reason about (as one would with a mathematical formula). Although this
approach does permit \emph{aliasing}, the consequences are benign because the
result of any computation is a \emph{new} value, distinct from any used during
the calculation of that value.

\begin{figure}[tbp]
    \centering
    \begin{minted}[fontsize=\small]{ocaml}
let mul x y =
  if same_shape x y then
    let y = copy y in
    (_owl_mul (kind x) (numel x) x y y; y)
  else
    broadcast_op (_owl_broadcast_mul (kind x)) x y
    \end{minted}
    \caption{Implementation of Matrix Multiplication in Owl (OCaml).
        Note the `copy' for the result and the unsafe `\_owl\_mul' operation
    used to perform an in-place multiplication.}\label{fig:mat_mul_owl}
\end{figure}

However, these advantages come with some costs: constantly producing new values
is wasteful on memory (although the example given in
Figure~\ref{fig:mat_mul_copies} is only a $2 \times 2$ matrix, many real-world
datasets can contain up to gigabytes of data). A complex expression may create
many short-lived temporaries which would need to be reclaimed by a
garbage-collector (see Figure~\ref{fig:mat_mul_owl}). Libraries taking a
\emph{value-semantic} approach offer a dichotomy for a user wishing to
implement a new algorithm: either use the existing and safe primitives to build
an easy to reason about but slower, more memory-intensive algorithm, or use
escape-hatches (typically provided by most libraries, which permit in-place
modification of objects) to build faster, and more memory-efficient algorithms
which are harder to reason about.

\subsection{IHNIWTLM}
The title of this subsection\footnote{I Have No Idea What Those Letters Mean.}
illustrates the problem with the C++/FORTRAN side: readability (equational
and algebraic expressions) and ease of reasoning is sacrificed at
the altar of performance and efficiency.

Although escape-hatches do exist in value-semantic libraries, their use is
discouraged. Systematic consideration of performance requires lowering the
level of abstraction a programmer is working on. At this level, several
factors such as memory layout, allocation, re-use as well as cache behaviour
and parallelism become apparent. Of these, \textbf{memory allocation and
re-use} are relevant to linear-types and this thesis.

\begin{figure}[tbp]
    \begin{minted}[fontsize=\small]{fortran}
  program blasMatMul
  implicit none
  real*4 a(2,2), b(2,2), c(2,2)
C External from BLAS
  external dgemm
C Initialize in column major storage of Fortran
  data a/ 1,0,0,1/
  data b/ 4,1,1,2/
C            tfm  tfm  rowA colB K  alpha a lda  b  ldb beta c  ldc
  call dgemm('N', 'N', 2,   2,   2, 1.0,  a,  2, b, 2,  0.0, c, 2)
    \end{minted}
    \caption{One of \emph{several} BLAS (Fortran) routines for Matrix
    Multiplication.}\label{fig:fortran_blas}
\end{figure}

In Fortran (Figure~\ref{fig:fortran_blas}), data is typically allocated
statically (at compile time) so temporary storage for all intermediate values
must be managed by the programmer. While this approach leads to verbose and
less readable code, the explicitness is good for understanding the memory
concerns of the program, albeit at the expense of understanding what the
program is actually calculating.

On the other hand, C++ (with operator overloading) can end up looking fairly
readable. For safety and correctness, expressions are typically handled with
value-semantics. However, given \emph{extra} information about, aliasing
(Eigen, Figure~\ref{fig:cpp_eigen}) or usage of intermediate expressions
(uBLAS, Figure~\ref{fig:cpp_ublas}), the number of temporaries allocated can be
reduced and increased \emph{implicitly} to improve performance (remove
unnecessary allocations or re-calculations respectively). Further tricks to
improve performance include expression templates (building up an
expression-tree at compile time and then pattern-matching on it to produce
code) and lazy evaluation (only calculating a result when it is needed). These
will be discussed in more detail in Chapter~\ref{chap:related}.

It is important to note that should these annotations (in
Figures~\ref{fig:cpp_eigen} and~\ref{fig:cpp_ublas}) be wrong, the program's
behaviour is very likely to end up being undefined (like how \texttt{memcpy()}
for overlapping regions of memory is explicitly undefined in the POSIX and C
standards). Indeed, one of Fortran's strengths lies in assuming that references
cannot be aliased (with certain caveats) in more cases than C permits (this
informal, general statement comes with many nuances left for the interested
reader to pursue).

\begin{figure}[tp]
    \begin{minted}[fontsize=\small]{c++}
#include <iostream>
#include <Eigen/Dense>
using namespace std;
int main()
{
    Eigen::Matrix2d a,b,c;
    a << 1, 0, 0, 1; b << 4, 1, 1, 2; c << 0, 0, 0, 0;
    a * b; // new matrix
    c += a * b; // temporary for correctness in case of aliasing
    c.noalias() += a * b; // no temporaries
}
    \end{minted}
    \caption{Some examples of Matrix Multiplication in Eigen. Using expression
        templates (to be discussed later) and \emph{explicitly provided} aliasing
        information, Eigen can emit a single BLAS `dgemm'-like call for the last
	line, mirroring the Fortran example of
	Figure~\ref{fig:fortran_blas}.}\label{fig:cpp_eigen}
\end{figure}

\begin{figure}[tbp]
    \begin{minted}[fontsize=\small]{c++}
noalias(C) = prod(A, B);
// Preferable if T is preallocated
temp_type T = prod(B,C); R = prod(A,T);
prod(A, temp_type(prod(B,C));
prod(A, prod<temp_type>(B,C));
    \end{minted}
    \caption{Boost uBLAS example of Matrix Multiplication. Temporaries need to
        be marked as such to prevent unnecessary re-computation of
        values.}\label{fig:cpp_ublas}
\end{figure}

\section{Proposed Solution}\label{sec:prop_sol}

My proposed solution to this dichotomy (readability, ease of reasoning and
safety versus memory-efficiency) is a \emph{domain-specific language} (DSL),
called LT4LA (Linear Types for Linear Algebra). It is written in OCaml and
transpiles to OCaml. It offers readable, explicit management of aliasing,
read/write permissions, memory allocation, re-use and deallocation all with
automated checking: offering a safety net to catch the baby whilst swiftly
disposing of the bath water.  Although for expository and testing purposes I
have defined a concrete-syntax, a full implementation would make use of a
language's syntax-extension features (such as PPX for OCaml) to \emph{embed}
the DSL into the host language. Such an embedding is straightforward but fairly
tedious to implement. As a half-way point, I used compile-time code generation
to make the DSL's output available to OCaml for testing and evaluation.

Let us have a look at a few examples of functions we can write with linear
types.  We can define the canonical factorial function
(Figure~\ref{fig:ltfla_factorial}) and sum over an array
(Figure~\ref{fig:ltfla_sumarray}).

\begin{figure}[tp]
    \inputminted[linenos, fontsize=\small]{ocaml}{../test/examples/factorial.lt}
    \caption{Factorial function in LT4LA.}\label{fig:ltfla_factorial}
\end{figure}

\begin{figure}[tp]
    \inputminted[linenos, fontsize=\small]{ocaml}{../test/examples/sum_array.lt}
    \caption{Summing over an array in LT4LA.}\label{fig:ltfla_sumarray}
\end{figure}

The syntax is intended to resemble OCaml's, apart from the spurious
\ltfla{`!'}s found here and there (they are annotations to show that we can
use a value more than once). In Figure~\ref{fig:ltfla_sumarray}, we see
\ltfla{row} has type (\ltfla{'x arr}). More detailed explanations of
\emph{what and why} will be given in Chapter~\ref{chap:impl}, but for now, it
is enough to know it means we can only \emph{read} from \ltfla{row}, and not
write to it. If we did try to write to or free \ltfla{row}, say by adding
another line as shown in Figure~\ref{fig:ltfla_univ}, would get a helpful error
message, as shown.

\begin{figure}[tp]
    \begin{minted}[linenos, firstnumber=7, fontsize=\small]{ocaml}
let row = row[i] := x1 in (* or *) let () = free row in
(* Could not show equality:                                        *)
(*     z arr                                                       *)
(* with                                                            *)
(*     'x arr                                                      *)
(*                                                                 *)
(* Var 'x is universally quantified                                *)
(* Are you trying to write to/free/unshare an array you don't own? *)
(* In test/examples/sum_array.lt, at line: 7 and column: 19        *)
    \end{minted}
    \caption{Attempting to write to or free a read only array in
    LT4LA.}\label{fig:ltfla_univ}
\end{figure}

Now suppose we were trying to square a matrix, using a `dgemm' like BLAS
routine (called `simple\_dgemm'\footnote{For the purposes of this example, I
assume `simple\_dgemm' has type \ltfla{'x . 'x mat --o 'y .  'y mat --o z mat
--o ('x mat * 'y mat) * z mat}.} in Figures~\ref{fig:ltfla_square}
and~\ref{fig:ltfla_free}) which takes two read-only matrices and a third matrix
it can write to and performs $C := AB + C$. How would we use such a routine to
square a matrix, assuming $C=0$ initially?  Surely this would break linearity,
since $A$ would be have to be given to the function twice, and we can only do
so once?

We can use a special primitive called \ltfla{share} to produce (more) read-only
\emph{aliases} of any matrix. We then pass these into the function, and it
works as expected. Once the squared matrix has been obtained, we may not want
the original any more, and thus decide to free it. Before we do so, we must
first \ltfla{unshare} any read-only aliases that exist -- in this context,
\ltfla{unshare} swallows them and gives back a single, read-write handle.

\begin{figure}[tp]
    \begin{minted}[linenos, fontsize=\small]{ocaml}
let (a1, a2) = share _ a in
let ((a1, a2), c) = simple_dgemm _ a1 _ a2 c in
let a = unshare _ a1 a2 in
let () = free a in c
    \end{minted}
    \caption{Squaring a matrix in LT4LA (assuming \ltfla{c} is initially
        0).}\label{fig:ltfla_square}
\end{figure}

If we tried to free one of the read-only aliases instead of
\ltfla{unshare}-ing, then we would get the error shown in
Figure~\ref{fig:ltfla_free}. Briefly, a \ltfla{z arr} is a read-write array,
everything else (\ltfla{'x arr} or \ltfla{_ s arr}) is read-only. The types of
\ltfla{free}, \ltfla{share} and \ltfla{unshare} (see
Appendix~\ref{chap:primitives}) are set up so that you can only free something
when you have read-write access to it, guaranteed by linearity to be the only
name in scope with this capability (aliases can only be read-only). Conversely,
if we \emph{forgot} to \ltfla{free a}, we would also get \texttt{Variable a not
used} error.

Another way in which `simple\_dgemm' may be misused is by instantiating its
informal description of $C := AB + C$ with $B = C = A$ and so mistakenly
concluding that it computes $A := A^2 + A$ in-place. However, the type of
\ltfla{share} prevents this as well -- \ltfla{let (a11, a12) = share _ a1 in
simple_dgemm _ a11 _ a12 a2} would result in an error similar to the one shown
in Figure~\ref{fig:ltfla_free}.

\begin{figure}[tp]
    \begin{minted}[linenos, fontsize=\small]{ocaml}
let (a1, a2) = share _ a in
let ((a1, a2), c) = simple_dgemm _ a1 _ a2 c in
let () = free a1 in c
(* Error:                                                        *)
(* Could not show equality:                                      *)
(*     z arr                                                     *)
(* with                                                          *)
(*     z s arr                                                   *)
(*                                                               *)
(* Could not show z and z s are equal.                           *)
(* Are you trying to write to/free an array before unsharing it? *)
(* In test.lt, at line: 3 and column: 17                         *)
    \end{minted}
    \caption{Attempting to free a read-only alias of matrix.}\label{fig:ltfla_free}
\end{figure}


\section{Further Reading and Theory}\label{sec:further}

% Just as no Scottish MP's initial speech in the House of Commons would be
% complete without some claim or mention to Rabbie Burns,

No exposition of linear types would be complete without a mention of Girard's
Linear Logic~\cite{girard}. As mentioned in the Stanford Encyclopedia of
Philosophy, it is ``a refinement of classical and intuitionistic logic. Instead
of emphasizing truth, as in classical logic, or proof, as in intuitionistic
logic, linear logic emphasizes the role of formulas as resources.'' A walk from
logic to programming along the well-trodden Curry-Howard bridge brings us to
linear types~\cite{wadler}.

For the category theory inclined reader, the !-operator (sometimes, for reasons
elided here, called \emph{exponentiation}) forms a co-monad; for the rest of
us, this entails two (rather simple) facts about a value you can use any number
of times: you can (1) can use it once (co-unit), and (2) can pass it to many
contexts that will use it many times (co-multiply).

More generally, by annotating variables in the context with their usage (when
implementing a type-checker for a linearly typed language), we can express the
rules of \emph{substructural} (including affine, relevant and ordered type
systems) under the more general framework of \emph{co-effects}~\cite{petricek}.

Stepping further back, both the practice and theory behind resource-aware
programming has made visible progress in the past few years.  On the
programming side, we have Linear Haskell, Rust and Idris (experimental). On the
research side, we have Resource Aware ML~\cite{hoffmann} and the tantalising
promise of integrating linear and dependent types~\cite{atkey}.

\clearpage%
\section{Summary}

I have given an \emph{intuitive} exposition of linear types with
\emph{fractional-capabilities}~\cite{boyland}, emphasising small, but
illustrative and practical code examples, leaving \emph{what} is going on and
\emph{why} it works as details for the next chapter. I have shown that it is
possible to solve the dichotomy of readability, ease of reasoning and safety
versus memory-efficiency by providing explicit and automatically checked
management of aliasing, read/write permissions, memory allocation, re-use and
deallocation with a \emph{type system}. This prevents a whole class of errors
that can occur with lower-level languages \emph{at compile time}. I
demonstrated these features using the context of linear-algebra libraries as as
a specific example. In the next chapter, I will show that these features can be
further applied to the problem of \emph{matrix expression compilation}. I will
also explain how to express and implement them so that they can be provided and
used \emph{as a library}.
