\section{Discussion and Related Work}\label{sec:discussion_related_work}

\subsection{Finding Bugs in SymPy's Output}\label{subsec:finding_bugs}

Prior to this project, we had little experience with linear algebra libraries
or the problem of matrix expression compilation. As such, we based our initial
\lang\ implementation of a Kalman filter using BLAS and LAPACK, on a popular
GitHub gist of a Fortran implementation, one that was \emph{automatically
generated} from SymPy's matrix expression compiler~\cite{rocklin_thesis}.

Once we translated the implementation from Fortran to \lang, we attempted to
compile it and found that (to our surprise) it did not type-check. This was
because the original implementation contained incorrect aliasing, unused
variables and unnecessary temporaries, and did not adhere to Fortran's
read/write permissions (with respect to \texttt{intent} annotations
\texttt{in}, \texttt{out} and \texttt{inout}) all of which were now highlighted
by \lang's type system.

The original implementation used 6 temporaries, one of which was immediately
spotted as never being used due to linearity. It also contained two variables
which were marked as \texttt{intent(in)} but would have been written over by
calls to `gemm', spotted by the fractional-capabilities feature. Furthermore,
it used a matrix \emph{twice} in a call to `symm', once with a read permission
but once with a \emph{write} permission.  Fortran assumes that any parameter
being written to is not aliased and so this call was not only incorrect, but
illegal according to the standard, both aspects of which were captured by
linearity and fractional-capabilities.

Lastly, it contained another unnecessary temporary, however one that was not
obvious without linear types. To spot it, we first performed live-range
splitting (checked by linearity) by hoisting calls to \highl{freeM} and then
annotated the freed matrices with their dimensions.  After doing so and
spotting two disjoint live-ranges of the same size, we replaced a call to
\highl{freeM} followed by allocating call to \highl{copy} with one, in-place
call to \highl{copyM_to}. We believe the ability to boldly refactor code which
manages memory is good evidence of the usefulness of linearity as a tool for
programming.

\subsection{Related Work}

Using linear types for BLAS routines is a particularly good domain fit (given
the implicit restrictions on aliasing arguments), but is under-explored in the
academic literature. Linear algebra libraries written in Rust take advantage of
the distinction that Rust's type system offers between mutable views/references
to arrays, offering a \emph{seemingly} different approach to the
fractional-permissions based one we took in this paper.  However, recent
research\cite{weiss} suggests that Rust's borrow-checker could be explained in
simpler terms using \emph{fractional-permissions}; in this light, our paper
neatly bridges the gap by demonstrating the theoretical and practical
simplicity of this way of thinking.

Linear Haskell\cite{bernardy2017linear}, takes a slightly different definition
of linearity, that is one on \emph{arrows} as opposed to \emph{kinds}: for $f :
a \multimap b$, if $f u$ is used exactly once \emph{then} $u$ is used exactly
once. Whilst this has the advantage of being backwards-compatible, \lang's
approach of compile-time code-generation enables using linearity \emph{as a
library}, whilst the use of fractions gives extra control over aliasing not
available in Linear Haskell.

In general, using substructural types to express array computations is not
particularly new\cite{scholz,henriksen,bernardy2016}, but these approaches
limit recursion and higher-order functions, and have special constructs to
enable \emph{implementation} of efficient BLAS-like libraries rather than
enforcing correct \emph{usage}.

\subsection{Simplicity and Further Work}

We are pleasantly surprised at how simple the overall design and implementation
of \lang\ is, given its expressive power and usability.  So simple in fact,
that fractions, a convenient theoretical abstraction until this point, could be
implemented by restricting division and multiplication to be by 2 only
\cite{boyland2003}, thus turning any required arithmetic into unification.

Indeed, the focus on getting a working prototype early on (so that we could
test it with real BLAS/LAPACK routines as soon as possible) meant that we only
added features to the type system when it was clear that they were absolutely
necessary: these features were !-types and value-restriction for the
\highl{Many} constructor. 

Going forwards, one may wish to eliminate even more runtime errors from \lang,
by extending its type system. For example, we could have used existential types
to statically track pointer identities\cite{morrisett}, or parametric
polymorphism.

We could also attempt to catch mismatched dimensions at compile time as well.
While this could be done with generative phantom types\cite{abe2015simple},
using dependent types may offer more flexibility in \emph{partitioning}
regions\cite{space_monads} or statically enforcing dimensions related
constraints of the arguments at compile-time.  ATS\cite{cui2005ats} is just
such a language which combines dependent and linear types; although it provides
BLAS bindings, it does not aim to provide aliasing restrictions as demonstrated
in this paper.

Taking this idea one step even further, since matrix dimensions are typically
fixed at runtime, we could \emph{stage} \lang\ programs and compile matrix
expressions using more sophisticated algorithms\cite{barthels}. However, it is
worth noting that without care, such algorithms\cite{rocklin_thesis}, usually
based on graph-based, ad-hoc dataflow analysis, can produce erroneous output
which would not get past a linear type system with fractions.

We also think that this concept (and the general design of its implementation)
need not be limited to linear algebra: we could conceivably `backport' this
idea to other contexts that need linearity (concurrency, single-use
continuations, zero-copy buffer, streaming I/O) or combine it with dependent
types to achieve even more expressive power to split up a single block of
memory into multiple regions in an arbitrary manner\cite{space_monads}.
